{"cells":[{"cell_type":"markdown","metadata":{"id":"_CYOJl436lRd"},"source":["# Project 2: A minimal model training experiment\n","\n","**Goal**:\n","\n","- Create a [PyTorch LightningModule](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html) named `ImageClassifier` that holds a convolutional network with ResNet18 backbone.\n","- Learn to prepare the dataset and dataloader to be compatible with the model.\n","- Understand a minimal set of components of a model training pipeline: loss, optimizer, metrics, training loop.\n","- Assemble the components to train a model using the dataset class and the dataloader created in the previous object.\n","- Learn how to visualize model predictions.\n","- Understand the concept of fine-tuning and the benefits of starting from a pre-trained model. (may move to the next project)\n","- Understand the benefits and options of a dataloader. (may move to the next project)\n","\n","**Acceptance Criteria**:\n","\n","- Implement a test that checks a simple ImageClassifier can be predict on an image that has the correct shape.\n","- The `ImageClassifier` can be trained on the CIFAR10 dataset, showing decreasing loss and accuracy for several epochs.\n","\n","**Resources**:\n","\n","- If you want to use docker to run code in this notebook, `pytorch/pytorch:1.12.0-cuda11.3-cudnn8-runtime` is a good choice.\n","\n","## Step 1: Create a model using `LightningModule`\n","\n","`LightningModule` is a convenient and structured way to implement a PyTorch model, as well as its training and validation behaviors. For more information, please refer to the [documentation](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html) for `LightningModule`."]},{"cell_type":"markdown","source":["### Installation"],"metadata":{"id":"ToKB6z5bPsYe"}},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"7GrSifYKkSgj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"24qmzkhw6lRf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660774520917,"user_tz":300,"elapsed":4241,"user":{"displayName":"Zhangzhang Si","userId":"05784349039024700350"}},"outputId":"d007b578-ff45-4ce3-e381-ac26deefe1e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n","Requirement already satisfied: pytorch-lightning==1.6.4 in /usr/local/lib/python3.7/dist-packages (1.6.4)\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.9.3)\n","Requirement already satisfied: mlflow in /usr/local/lib/python3.7/dist-packages (1.28.0)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.4) (2022.7.1)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.4) (6.0)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.4) (2.9.1)\n","Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.4) (3.17.3)\n","Requirement already satisfied: pyDeprecate>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.4) (0.3.2)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.4) (4.1.1)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.4) (21.3)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.4) (4.64.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.4) (1.21.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (3.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (2.23.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning==1.6.4) (3.0.9)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1->pytorch-lightning==1.6.4) (1.15.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (57.4.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (1.8.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (1.2.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (0.4.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (0.37.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (0.6.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (1.47.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (1.35.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (3.4.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.4) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.4) (4.9)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.4) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.4) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.4) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.4) (3.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.4) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.4) (3.2.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: pytz<2023 in /usr/local/lib/python3.7/dist-packages (from mlflow) (2022.2)\n","Requirement already satisfied: alembic<2 in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.8.1)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (7.1.2)\n","Requirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (3.1.27)\n","Requirement already satisfied: databricks-cli<1,>=0.8.7 in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.17.1)\n","Requirement already satisfied: scipy<2 in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.7.3)\n","Requirement already satisfied: querystring-parser<2 in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.2.4)\n","Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.0)\n","Requirement already satisfied: prometheus-flask-exporter<1 in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.20.3)\n","Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.4.2)\n","Requirement already satisfied: pandas<2 in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.5)\n","Requirement already satisfied: sqlalchemy<2,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.4.40)\n","Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.4)\n","Requirement already satisfied: docker<6,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (5.0.3)\n","Requirement already satisfied: gunicorn<21 in /usr/local/lib/python3.7/dist-packages (from mlflow) (20.1.0)\n","Requirement already satisfied: Flask<3 in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.4)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic<2->mlflow) (5.9.0)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic<2->mlflow) (1.2.1)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.8.10)\n","Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.4.0)\n","Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from docker<6,>=4.0.0->mlflow) (1.3.3)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask<3->mlflow) (2.11.3)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask<3->mlflow) (1.1.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython<4,>=2.1.0->mlflow) (4.0.9)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow) (5.0.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask<3->mlflow) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2->mlflow) (2.8.2)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter<1->mlflow) (0.14.1)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy<2,>=1.4.0->mlflow) (1.1.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (4.0.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (6.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (1.8.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (22.1.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (1.2.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (0.13.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (2.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (1.3.1)\n"]}],"source":["# Install the dependencies:\n","!pip install torch torchvision pytorch-lightning==1.6.4 torchmetrics mlflow\n","# If the above installation fails, try:\n","# !pip install torch==1.12.0 torchvision==0.13.0 pytorch-lightning==1.6.4 torchmetrics mlflow"]},{"cell_type":"markdown","source":["### A simple image classifier module"],"metadata":{"id":"DJnZSK8uPw5r"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_nCvbMV6lRg"},"outputs":[],"source":["import pytorch_lightning as pl\n","from torchvision.models import resnet18\n","\n","class ImageClassifier(pl.LightningModule):\n","    def __init__(self):\n","        super().__init__()\n","        self.net = resnet18(num_classes=10, weights=None)\n","    \n","    def forward(self, x):\n","        return self.net(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rvvzdAa66lRg"},"outputs":[],"source":["model = ImageClassifier()"]},{"cell_type":"markdown","metadata":{"id":"OGnw8Gkn6lRg"},"source":["### Testing the forward path of the model\n","\n","As a trainable function, the model is callable. The model's forward path (as defined in `forward()`) is the normal execution of the function. We can test this by passing an image to the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJNomTn36lRg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660774522447,"user_tz":300,"elapsed":7,"user":{"displayName":"Zhangzhang Si","userId":"05784349039024700350"}},"outputId":"1a5a880a-595b-43bd-ac9e-fde457d56f2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model is callable.\n"]}],"source":["assert callable(model), \"The model is not a callable object.\"\n","print(\"Model is callable.\")"]},{"cell_type":"markdown","metadata":{"id":"feJOhW_q6lRg"},"source":["#### What is a valid input?\n","\n","To test the forward path of the model, we need to pass a valid input. The input should be a tensor of shape `(b, 3, 32, 32)` where `b` is the batch size (an integer).\n","\n","**Your Task**: Fix the code below to pass the test.\n","\n","**Tips:** `torch.from_numpy()` can be used to convert a numpy array to a tensor.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrVC6KfG6lRh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660774522447,"user_tz":300,"elapsed":6,"user":{"displayName":"Zhangzhang Si","userId":"05784349039024700350"}},"outputId":"d9ee89c7-7fd2-4255-9d2c-ac6faea23ddb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test passed\n"]}],"source":["import numpy as np\n","import torch\n","\n","# TODO: The test is broken. Please fix it.\n","def test_model_can_predict_on_a_random_image():\n","    # input_image = np.ones(shape=(3, 224, 224), dtype=np.float32)\n","    input_image = torch.from_numpy(np.ones(shape=(1, 3, 224, 224), dtype=np.float32))\n","    output = model(input_image)  # run the model on the input image\n","    assert output.shape == (1, 10)  # is the output shape correct?\n","\n","\n","test_model_can_predict_on_a_random_image()\n","print(\"Test passed\")"]},{"cell_type":"markdown","metadata":{"id":"M36V_zvH6lRh"},"source":["## Step 2: Prepare the dataset and data loader\n","\n","You have two choices here and either way would work.\n","\n","1. Use the `CIFAR10` dataset class provided by `torchvision`. This is easier.\n","   ```\n","   from torchvision.datasets import CIFAR10\n","\n","   cifar10_train = CIFAR10(train=True, download=True, root=\"./cifar10\")\n","   cifar10_val = CIFAR10(train=False, download=True, root=\"./cifar10\")\n","   ```\n","2. Or, reuse the `CustomDataset` from the previous object."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jzw5sJTF6lRh"},"outputs":[],"source":["# TODO: use `torchvision.datasets.CIFAR10` or copy over the code from the previous project.\n","# TODO: inspect the dataset by print out one example from it.\n","# TODO: Make sure the image is a Tensor or np.array (so that the data loader can turn it into\n","#   a Tensor-typed image batch), rather than a PIL image.\n","from torchvision.datasets import CIFAR10\n","from torchvision import transforms\n","\n","\n","cifar10_train = CIFAR10(...)\n","cifar10_val = CIFAR10(...)"]},{"cell_type":"markdown","source":["To make the dataset compatible with PyTorch models, if you use `torchvision.datasets.CIFAR10`, the image needs to a `Tensor` rather than a PIL image. Please add `transform=transforms.ToTensor` when calling the class to creating the dataset object. More information about transforms can be found [here](https://pytorch.org/vision/stable/transforms.html)."],"metadata":{"id":"dBg5na6hOsoF"}},{"cell_type":"markdown","metadata":{"id":"v3lAVH0D6lRi"},"source":["## Step 3: Training the model\n","\n","#### Loss\n","\n","A loss is a function that takes the model's output (also called predictions) and the ground truth (also called \"targets\") as input and returns a scalar value. The loss is used as the feedback mechantism to optimize the model.\n","\n","For classification, the loss is typically the cross-entropy. Use `torch.nn.functional.cross_entropy` when the model output are logits or `torch.nn.functional.nll_loss` when the outputs are probabilities (a.k.a. softmax). Loss functions can have requirements on the type (`dtype`) of input. `cross_entropy`, for example, expects targets to be \"Long\" or `np.int64`."]},{"cell_type":"markdown","metadata":{"id":"yGgrXk_26lRi"},"source":["**Your Task**: The following code almost works but there is a bug related to data type. Please fix it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmKA1tbx6lRi"},"outputs":[],"source":["from torch.nn import functional as F\n","\n","# To simplify the code, assume there are 3 image classes.\n","y_pred = torch.from_numpy(np.array([[0, 0, 5]], dtype=np.float32))\n","y_true = torch.from_numpy(np.array([1], dtype=np.float32))\n","print(F.cross_entropy(y_pred, y_true))"]},{"cell_type":"markdown","metadata":{"id":"0oIggu786lRi"},"source":["**Your Task**: Can you make the loss lower? Try changing `y_pred` in the next cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7G6aUrGf6lRi"},"outputs":[],"source":["# TODO: change y_pred to make the loss lower.\n","y_pred = torch.from_numpy(np.array([[0, 0, 5]], dtype=np.float32))\n","y_true = torch.from_numpy(np.array([1], dtype=int))\n","print(F.cross_entropy(y_pred, y_true))"]},{"cell_type":"markdown","metadata":{"id":"p6qi1Za06lRi"},"source":["#### Optimizer\n","\n","The gradient over model parameters (a.k.a. model weights) is computed of the loss function, and informs how the model parameters should be updated. The specifics of how the model is updated is handled by an optimizer.\n","\n","An optimizer can be created by passing the model parameters (`model.parameters()`) to the optimizer constructor, as well as learning rate, momentum, and other hyper-parameters. Below is a minimal example of using an optimizer.\n","\n","```{python}\n","for input, target in dataset:\n","    optimizer.zero_grad()\n","    output = model(input)\n","    loss = loss_fn(output, target)\n","    loss.backward()\n","    optimizer.step()\n","```\n","\n","When using `LightningModule`, this is taken care of by `LightniningModule` under the hood so you don't have to write the boilerplate code as below (pseudo-code):\n","\n","```\n","# put model in train mode and enable gradient calculation\n","model.train()\n","torch.set_grad_enabled(True)\n","\n","outs = []\n","for batch_idx, batch in enumerate(train_dataloader):\n","    loss = training_step(batch, batch_idx)\n","    outs.append(loss.detach())\n","\n","    # clear gradients\n","    optimizer.zero_grad()\n","\n","    # backward\n","    loss.backward()\n","\n","    # update parameters\n","    optimizer.step()\n","```\n","\n","For more details, please refer to [the documentation for PyTorch optimizers](https://pytorch.org/docs/stable/optim.html) and [the documentation for `LightningModule`](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html)."]},{"cell_type":"markdown","metadata":{"id":"MDZHjqLC6lRj"},"source":["#### Training Step\n","\n","We can start specifying the training behavior to the model by adding the `training_step` method in the `ImageClassifier` class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xvzon5A56lRj"},"outputs":[],"source":["class ImageClassifier(pl.LightningModule):\n","    def __init__(self):\n","        super().__init__()\n","        self.net = resnet18(num_classes=10, weights=None)\n","        self.automatic_optimization = False  # \n","    \n","    def forward(self, x):\n","        return self.net(x)\n","    \n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=0.001)\n","    \n","    def loss(self, y_hat, y):\n","        return F.cross_entropy(y_hat, y)\n","    \n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self(x)\n","        loss = self.loss(y_hat, y)\n","        \n","        return {\"loss\": loss}"]},{"cell_type":"markdown","metadata":{"id":"0Zg-OIOK6lRj"},"source":["**Your task**: Implement the following unit test. Use the data loader to get a batch of data.\n","Then, call the `training_step` and assert that the return value is a dictionary that looks\n","like this: `{'loss': 0.5}`. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-W4KYT26lRj"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","\n","def test_training_step_works():\n","    model = ImageClassifier()\n","    # TODO: Use the data loader to get a batch of data.\n","    #    Then, call the `training_step` and assert that the return value is a dictionary that looks\n","    #    like this: {'loss': 0.5}.\n","    # ...\n","    \n","\n","test_training_step_works()"]},{"cell_type":"markdown","metadata":{"id":"UlDHbxTk6lRj"},"source":["### Metrics\n","\n","A metric is a function that takes the model's output and the ground truth as input and returns a scalar value. The metric is used to evaluate the model's performance. It is mainly used to monitor the training progress and does not directly influence the model's training behavior. However, it can influence model selection and early stopping.\n","\n","For classification, the metric is typically the accuracy. We will use `torchmetrics.Accuracy` in this project."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k6IUxKhd6lRj"},"outputs":[],"source":["from torchmetrics import Accuracy\n","\n","# Again, for simplicity, assume there are only 3 imge classes.\n","y_pred = torch.from_numpy(np.array([[0, 0, 5], [2, -1, 1]], dtype=np.float32))\n","# The following line is optional and shouldn't change the result.\n","# y_pred = F.softmax(y_pred, dim=1)\n","y_true = torch.from_numpy(np.array([1, 0], dtype=int))\n","acc = Accuracy(num_classes=3)\n","acc.update(y_pred, y_true)\n","print(\"Accuracy:\", acc.compute())"]},{"cell_type":"markdown","metadata":{"id":"i_yEGoeL6lRj"},"source":["### Training loop\n","\n","A training loop manages the training process that iteratively passes batches of training examples to the model and running the training step. \n","\n","The training steps are organized into \"epochs\", where an epoch can be a single pass through the entire training dataset, or it can be simply a predefined number of training steps. At the end of an epoch, the model is used predict on the validation dataset and calculate accuracy metrics.\n","\n","To do this, we need to add `validation_step` and `validation_epoch_end` methods and accuracy metrics to the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VN2NnA7r6lRk"},"outputs":[],"source":["from torchmetrics import Accuracy\n","\n","\n","class ImageClassifier(pl.LightningModule):\n","    def __init__(self):\n","        super().__init__()\n","        self.net = resnet18(num_classes=10, weights=None)\n","        self.val_accuracy = Accuracy(num_classes=10)\n","    \n","    def forward(self, x):\n","        return self.net(x)\n","    \n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=0.0001)\n","    \n","    def loss(self, y_hat, y):\n","        return F.cross_entropy(y_hat, y)\n","    \n","    def training_step(self, batch, batch_idx=None):\n","        x, y = batch\n","        y_hat = self(x)\n","        loss = self.loss(y_hat, y)\n","\n","        # Note: The return value can be None, a loss tensor, or a dictionary with a \"loss\" key.\n","        return {\"loss\": loss}\n","    \n","    def validation_step(self, batch, batch_idx=None):\n","        x, y = batch\n","        y_hat = self(x)\n","        self.val_accuracy.update(preds=y_hat, target=y)\n","        val_loss = self.loss(y_hat, y)\n","        return {\"loss\": val_loss}"]},{"cell_type":"markdown","metadata":{"id":"9w3RlaDl6lRk"},"source":["### Start model training\n","\n","**Your Task**: In the next cells, fill out the details in the training loop implementation with the help of the following pseudo-code examples.\n","\n","The training loop can be described on a high level:\n","\n","```{python}\n","for epoch in epochs:\n","    for train_step in train_steps_per_epoch:\n","        # TODO: loop over the training dataset and run training steps\n","        pass\n","    \n","    # TODO: loop over the validation dataset and run validation steps.\n","```\n","\n","Pseudo code for fetching the next batch of data for training or validation:\n","\n","```{python}\n","batch = next(my_data_iterator)\n","```\n","\n","Pseudo code for back-propagation to update the model weights:\n","\n","```{python}\n","optimizer.zero_grad()\n","your_training_loss.backward()\n","optimizer.step()\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9jahe3W6lRk"},"outputs":[],"source":["from tqdm import tqdm\n","\n","epochs = 6\n","train_steps_per_epoch = 100\n","val_steps_per_epoch = 10\n","\n","train_dataloader = DataLoader(cifar10_train, batch_size=16)\n","val_dataloader = DataLoader(cifar10_val, batch_size=16)\n","\n","\n","def get_infinite_data_iterator(dataloader):\n","  while True:\n","    for batch in iter(dataloader):\n","      yield batch\n","\n","\n","#\n","# The training loop:\n","#\n","def training_loop(model, train_dataloader, val_dataloader):\n","    train_data_iterator = get_infinite_data_iterator(train_dataloader)\n","    optimizer = model.configure_optimizers()\n","    for epoch in range(epochs):\n","        train_losses = []\n","        train_steps = tqdm(list(range(train_steps_per_epoch)))\n","        for train_step in train_steps:\n","            # TODO: loop over the training dataset and run the training step.\n","            \n","            # TODO: Back-prop and update model's weights.\n","            \n","            # TODO: append the training loss value to the list `train_losses`.\n","\n","            avg_train_loss = np.mean(train_losses)\n","            train_steps.set_postfix({\"loss\": avg_train_loss})\n","        print(f\"epoch {epoch}: train loss = {avg_train_loss:.5f}\")\n","        \n","        # TODO: loop over the validation dataset and run validation steps.\n","        # Just using the first few examples from the validation set.\n","        val_data_iterator = iter(val_dataloader)\n","        val_losses = []\n","        for val_step in range(val_steps_per_epoch):\n","          # TODO: loop over the validation dataset and run the validation step.\n","          \n","          # TODO: append the validation loss value to the list `val_losses`.\n","        avg_val_loss = np.mean(val_losses)\n","        val_accuracy = model.val_accuracy.compute()\n","        print(f\"epoch {epoch}: val loss = {avg_val_loss:.5f}, accuracy = {val_accuracy:.3f}\")\n","\n","\n","model_v1 = ImageClassifier()\n","training_loop(model_v1, train_dataloader, val_dataloader)"]},{"cell_type":"markdown","metadata":{"id":"2L52pKqO6lRk"},"source":["## Step 4: Visualize model predictions\n","\n","Having loss going down and accuracy going up is nice. But to have the peace of mind that the model is working and is indeed better, it is helpful to visualize the model's predictions.\n","\n","**Your Task**: Complete the following cells to visualize the model's predictions.\n","\n","First, visualize predictions from a randomly initialized model:"]},{"cell_type":"code","source":["from matplotlib import pyplot as plt\n","\n","\n","def show_images(images, labels: list = None, ncols: int = 8):\n","    nrows = (len(images) + ncols - 1) // ncols\n","    fig = plt.figure(figsize=(2 * ncols, 2 * nrows), linewidth=10, edgecolor=\"#04253a\")\n","    for i in range(len(images)):\n","        plt.subplot(nrows, ncols, i + 1)\n","        plt.imshow(images[i])\n","        if labels is not None:\n","          plt.title(labels[i])\n","        plt.axis('off')\n","    # fig.set_facecolor(\"#e1ddbf\")\n","    return fig\n","\n","\n","def predict_and_show(model, output_path=None):\n","    batch = next(iter(val_dataloader))\n","    images, labels = batch\n","    softmax = model(images)\n","    softmax = softmax.detach().numpy()\n","    predicted_labels = np.argmax(softmax, axis=-1)\n","    CLASSES = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\",\n","              \"horse\", \"ship\", \"truck\"]\n","    predicted_label_texts = [f\"{CLASSES[pred]}({CLASSES[correct]})\" for pred, correct in zip(predicted_labels, labels)]\n","    fig = show_images(images.numpy().transpose(0, 2, 3, 1), predicted_label_texts)\n","    if output_path:\n","        fig.savefig(output_path)\n","\n","\n","# TODO: use an un-trained model to predict on a few images from the validation set.\n","#   Use the skill learned in project 1 to display a batch of images.\n","randomly_initialized_model = ImageClassifier()\n"],"metadata":{"id":"cwqQIpE1HRRu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N2tcntIz6lRk"},"source":["Next, take the trained model and visualize its predictions on some image examples:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04I1qHC66lRk"},"outputs":[],"source":["# TODO: use the trained model (model_v1) to predict on a few images from the validation set.\n","#   Do predictions look better than the untrained, randomly initialized model?"]},{"cell_type":"markdown","source":["## Step 5: Using the `Trainer` provided by Pytorch-Lightning\n","\n","For commonly used computer vision models, it is often enough to the `Trainer` provided by Pytorch-Lightning to handle the training loop. This way, you can run model training with just a couple lines of code.\n","\n","### Callbacks\n","\n","For finer controller of the training loop behaviors, you can create a `Callback` subclass, and fill out `on_train_batch_end`, `on_validation_batch_end`, `on_validation_epoch_end` etc.\n"],"metadata":{"id":"lZPQY-iOpfj7"}},{"cell_type":"code","source":["from pytorch_lightning.callbacks import Callback\n","\n","\n","class ImageClassifierCallback(Callback):\n","\n","    def __init__(self, save_prediction_examples=False, *args, **kwargs):\n","      super().__init__(*args, **kwargs)\n","      self.current_epoch = 0\n","      self.save_prediction_examples = save_prediction_examples\n","    \n","    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n","        super().on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx)\n","        loss = outputs[\"loss\"]\n","        self.log(name=\"loss\", value=loss.item(), on_step=True)\n","    \n","    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n","        super().on_validation_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx)\n","        loss = outputs[\"loss\"]\n","        self.log(name=\"loss\", value=loss.item(), on_step=True)\n","        self.log(name=\"acc\", value=pl_module.val_accuracy.compute(), on_step=True)\n","        return {\"loss\": loss}\n","    \n","    def on_validation_epoch_end(self, trainer, pl_module, *args, **kwargs):\n","        super().on_validation_epoch_end(trainer, pl_module, *args, **kwargs)\n","        avg_acc = pl_module.val_accuracy.compute()\n","        self.log('val_acc', avg_acc, prog_bar=True)\n","        print(\"val_accuracy:\", avg_acc)\n","        if self.save_prediction_examples:\n","            pass  # not implemented\n","        self.current_epoch += 1"],"metadata":{"id":"gKDFcBHISzcT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pytorch_lightning import Trainer, seed_everything\n","\n","# Optional: freeze the random seed.\n","seed_everything(42, workers=True)\n","trainer = Trainer(deterministic=True, limit_train_batches=100, limit_val_batches=10, max_epochs=5, callbacks=[ImageClassifierCallback()])\n","model_v2 = ImageClassifier()\n","trainer.fit(model_v2, train_dataloader, val_dataloader)"],"metadata":{"id":"VdmMknSdpdZK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict_and_show(model_v2)"],"metadata":{"id":"dzn5qozRSgqR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 6 (optional): Track the model's progress using an experiment tracking tool\n","\n","Tracking model's quality during training is made easy by great experiment tracking tools like `mlflow`. With a small change to your training code, we will walk through how to use it to track your model's progress in learning.\n","\n","**Your task**: modify the `ImageClassifierCallback` class above, and make use of the `log_artifact` method of `MLFlowLogger` to log the figure produced by `predict_and_show` at the end of each epoch.\n","\n","Pseudo code example:\n","```\n","experiment_tracker = trainer.logger.experiment\n","experiment_tracker.log_artifact(run_id=..., local_path=...)\n","```"],"metadata":{"id":"yubA7ryEEI8Q"}},{"cell_type":"code","source":["from pytorch_lightning.loggers import MLFlowLogger\n","\n","trainer = Trainer(\n","    limit_train_batches=100,\n","    limit_val_batches=10,\n","    max_epochs=5,\n","    logger=MLFlowLogger(experiment_name=\"image classifier\", tracking_uri=\"./mlruns\"),\n","    callbacks=[ImageClassifierCallback(save_prediction_examples=True)]\n",")\n","\n","model_v3 = ImageClassifier()\n","trainer.fit(model_v3, train_dataloader, val_dataloader)"],"metadata":{"id":"lRUrKZmoEVPI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The tracking information is already saved by MLFlow. If you are running this on a local computer, you can type: \n","\n","```mlflow ui```\n","\n","and view the MLFlow GUI in your browser.\n","\n","If you are running this in colab or on a remote server, you can use the following code to visualize metrics and artifacts within this notebook."],"metadata":{"id":"yQOCwJOhrq2N"}},{"cell_type":"code","source":["import mlflow\n","\n","client = mlflow.client.MlflowClient(tracking_uri=\"./mlruns\")\n","exp = client.get_experiment_by_name(\"image classifier\")\n","runs = client.list_run_infos(exp.experiment_id, order_by=[\"tag.start_time DESC\"])\n","latest_run = runs[0]\n","latest_run"],"metadata":{"id":"gJwoMUwoqBKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_acc_history = client.get_metric_history(latest_run.run_id, key=\"val_acc\")\n","val_acc_history"],"metadata":{"id":"wZXqwHBYo0Dh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn\n","\n","steps = [m.step for m in val_acc_history]\n","val_acc = [m.value for m in val_acc_history]\n","\n","fig = seaborn.lineplot(steps, val_acc).set(title=\"val accuracy\", xlabel=\"step\", ylabel=\"value\")"],"metadata":{"id":"vzghEb-7sOW9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The image artifacts that contain prediction examples can also be retrieved and downloaded. Here we don't visualize them again because the images probably already show up in the cell above."],"metadata":{"id":"lo_yO-fmyfB1"}},{"cell_type":"code","source":["client.list_artifacts(run_id=latest_run.run_id)"],"metadata":{"id":"87fcYywPyRkO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You have completed the project!"],"metadata":{"id":"KmD5t_l562A5"}}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 ('kungfuu')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"3b6480ec1b87656350dd6ddf61559e3fd9d4b0710686fddab793795a6111e81d"}},"colab":{"name":"p2_training.ipynb","provenance":[{"file_id":"1M5lj-N9wP4ghkRiDleH5oynXx-83jekK","timestamp":1660927267849},{"file_id":"https://github.com/kungfuai/kungfuu-computer-vision/blob/main/course1-image-classification/p2_training.ipynb","timestamp":1660255101681}],"collapsed_sections":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}