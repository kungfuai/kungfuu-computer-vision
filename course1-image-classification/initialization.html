
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>4.1. Initialization of Model Weights &#8212; Evolve model training recipes for image classification</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="4.2. Effect of Optimizers" href="optimizer.html" />
    <link rel="prev" title="4. Knobs for Network Weights: Initialization and Optimizers" href="network_weights.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Evolve model training recipes for image classification</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="p1_dataset.html">
   1. Preparing an Image Dataset for Model Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="p2_training.html">
   2. A Minimal Model Training Experiment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="p3_dataloader.html">
   3. Efficent Data Loading for Model Training
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="network_weights.html">
   4. Knobs for Network Weights: Initialization and Optimizers
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     4.1. Initialization of Model Weights
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimizer.html">
     4.2. Effect of Optimizers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="arch.html">
   5. Effect of Model Architecture
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="aug.html">
   6. Effect of Image Pre-processing and Augmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="loss.html">
   7. Effect of Loss Function Parameters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="predictor.html">
   8. Wrapping up: Image Classifier as a Predictor Service
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="additional_topics.html">
   9. Optional: Selected topics on image classifiers
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="scale_experiments.html">
     9.1. Efficient Experiments: Scale to Hundreds of Experiments
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="self_supervised.html">
     9.2. Self Supervised Learning as a Pre-train Step for Image Classification
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="self_supervised/solo_learn.html">
       9.2.1. Self Supervised Learning and Linear Probing on CIFAR10
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="multitask.html">
     9.3. Multi-task Learning for Image Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bayesian.html">
     9.4. Bayesian Neural Networks for Image Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="unlearn.html">
     9.5. Adversarial Learning: Unlearn Questionable Correlations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="few_shot.html">
     9.6. Few Shot Learning: Cope with Small Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="explain.html">
     9.7. Transparancy: Explaining Model’s Prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="federated.html">
     9.8. Privacy: Federated Learning for Image Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="keys.html">
   10. Keys to Some Projects
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="p1_dataset_key.html">
     10.1. Project 1: Preparing an image dataset for model training
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="p2_training_key.html">
     10.2. Project 2: A minimal model training experiment
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Created at <a href="https://kungfu.ai">KUNGFU.AI</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://colab.research.google.com/github/kungfuai/kungfuu-computer-vision/blob/main/course1-image-classification/initialization.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/kungfuai/kungfuu-computer-vision"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/kungfuai/kungfuu-computer-vision/issues/new?title=Issue%20on%20page%20%2Finitialization.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/initialization.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#random-initialization">
   4.1.1. Random Initialization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-the-activations-and-gradients">
     4.1.1.1. Visualizing the Activations and Gradients
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#visualization-functions">
       4.1.1.1.1. Visualization Functions
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#initialization-functions">
       4.1.1.1.2. Initialization Functions
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-and-dataset">
     4.1.1.2. Model and Dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#see-the-effect-of-different-intialization-methods">
     4.1.1.3. See the Effect of Different Intialization Methods
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#constant-initialization">
       4.1.1.3.1. Constant Initialization
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#vanishing-gradients">
       4.1.1.3.2. Vanishing Gradients
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exploding-gradients-and-activation">
       4.1.1.3.3. Exploding Gradients and Activation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#xavier-initialization">
       4.1.1.3.4. Xavier Initialization
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kaiming-initialization">
       4.1.1.3.5. Kaiming Initialization
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#initialization-from-a-pre-trained-model-transfer-learning">
   4.1.2. Initialization from a Pre-trained Model: Transfer Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#showdown">
   4.1.3. Showdown
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initialize-from-a-pre-trained-model">
     4.1.3.1. Initialize from a Pre-trained Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-kaiming-initialization">
     4.1.3.2. Random Kaiming Initialization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-xavier-initialization">
     4.1.3.3. Random Xavier Initialization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     4.1.3.4. Constant Initialization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normal-distribution-initialization-small-variance">
     4.1.3.5. Normal Distribution Initialization (Small Variance)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normal-distribution-initialization-large-variance">
     4.1.3.6. Normal Distribution Initialization (Large Variance)
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Initialization of Model Weights</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#random-initialization">
   4.1.1. Random Initialization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-the-activations-and-gradients">
     4.1.1.1. Visualizing the Activations and Gradients
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#visualization-functions">
       4.1.1.1.1. Visualization Functions
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#initialization-functions">
       4.1.1.1.2. Initialization Functions
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-and-dataset">
     4.1.1.2. Model and Dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#see-the-effect-of-different-intialization-methods">
     4.1.1.3. See the Effect of Different Intialization Methods
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#constant-initialization">
       4.1.1.3.1. Constant Initialization
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#vanishing-gradients">
       4.1.1.3.2. Vanishing Gradients
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exploding-gradients-and-activation">
       4.1.1.3.3. Exploding Gradients and Activation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#xavier-initialization">
       4.1.1.3.4. Xavier Initialization
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kaiming-initialization">
       4.1.1.3.5. Kaiming Initialization
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#initialization-from-a-pre-trained-model-transfer-learning">
   4.1.2. Initialization from a Pre-trained Model: Transfer Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#showdown">
   4.1.3. Showdown
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initialize-from-a-pre-trained-model">
     4.1.3.1. Initialize from a Pre-trained Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-kaiming-initialization">
     4.1.3.2. Random Kaiming Initialization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-xavier-initialization">
     4.1.3.3. Random Xavier Initialization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     4.1.3.4. Constant Initialization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normal-distribution-initialization-small-variance">
     4.1.3.5. Normal Distribution Initialization (Small Variance)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normal-distribution-initialization-large-variance">
     4.1.3.6. Normal Distribution Initialization (Large Variance)
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="initialization-of-model-weights">
<h1><span class="section-number">4.1. </span>Initialization of Model Weights<a class="headerlink" href="#initialization-of-model-weights" title="Permalink to this headline">#</a></h1>
<p>The model’s weights can be initialized randomly or from a pre-trained model.</p>
<section id="random-initialization">
<h2><span class="section-number">4.1.1. </span>Random Initialization<a class="headerlink" href="#random-initialization" title="Permalink to this headline">#</a></h2>
<p>In practice, we seldom need to spend time tuning initialization parameters, because the default initialization method in deep learning libraries is often sufficient.</p>
<p>This <a class="reference external" href="https://www.deeplearning.ai/ai-notes/initialization/">blog post</a> from <a class="reference external" href="http://deeplearning.ai">deeplearning.ai</a> gives a great interative introduction of random initialization methods using toy examples. This <a class="reference external" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial4/Optimization_and_Initialization.html">executable tutorial</a> provides sample code and visualization tools to diagnose random initialization issues.</p>
<p>Xavier initialization <a class="footnote-reference brackets" href="#footcite-xavier" id="id1">1</a> (by Xavier Glorot) is a popular method and it has two main variants:</p>
<ul class="simple">
<li><p>Xavier uniform:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[W\sim U\left[-\frac{\sqrt{6}}{\sqrt{d_x+d_y}}, \frac{\sqrt{6}}{\sqrt{d_x+d_y}}\right]\]</div>
<ul class="simple">
<li><p>Xavier normal:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[W\sim \mathcal{N}\left(0,\frac{2}{d_x+d_y}\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(W\)</span> holds the weights of the layer, <span class="math notranslate nohighlight">\(d_x\)</span> and <span class="math notranslate nohighlight">\(d_y\)</span> are the input and output dimensions of the weight tensor, respectively. They are also called “fan in” and “fan out”. Initialized this way, the training process is made more stable through the following properties:</p>
<ol class="simple">
<li><p>The mean of the activations is close to zero</p></li>
<li><p>The variance of the activations is roughly the same across all layers</p></li>
<li><p>The variance of the gradients is roughly the same across all layers</p></li>
</ol>
<p>Later, Kaiming He proposed to multiply by 2 the variance of the Xavier initialization. This is the default initialization method in PyTorch for linear and conv layers (for example, see <a class="reference external" href="https://github.com/pytorch/pytorch/blob/v1.12.1/torch/nn/modules/conv.py#L144">how conv layers in PyTorch are initialized</a>).</p>
<section id="visualizing-the-activations-and-gradients">
<h3><span class="section-number">4.1.1.1. </span>Visualizing the Activations and Gradients<a class="headerlink" href="#visualizing-the-activations-and-gradients" title="Permalink to this headline">#</a></h3>
<p>First, let’s create visualization and initialization functions (adapted from <a class="reference external" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial4/Optimization_and_Initialization.html">example notebok</a>):</p>
<section id="visualization-functions">
<h4><span class="section-number">4.1.1.1.1. </span>Visualization Functions<a class="headerlink" href="#visualization-functions" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1">##############################################################</span>
<span class="k">def</span> <span class="nf">plot_dists</span><span class="p">(</span><span class="n">named_tensors</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;count&quot;</span><span class="p">,</span> <span class="n">use_kde</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">named_tensors</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">columns</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">columns</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>
    <span class="n">fig_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># for key in sorted(val_dict.keys()):</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">named_tensors</span><span class="p">:</span>
        <span class="n">key_ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">fig_index</span><span class="o">%</span><span class="k">columns</span>]
        <span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">key_ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="n">stat</span><span class="p">,</span>
                     <span class="n">kde</span><span class="o">=</span><span class="n">use_kde</span> <span class="ow">and</span> <span class="p">((</span><span class="n">tensor</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">-</span><span class="n">tensor</span><span class="o">.</span><span class="n">min</span><span class="p">())</span><span class="o">&gt;</span><span class="mf">1e-8</span><span class="p">))</span> <span class="c1"># Only plot kde if there is variance</span>
        <span class="n">key_ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> &quot;</span> <span class="o">+</span> <span class="p">(</span><span class="sa">r</span><span class="s2">&quot;(</span><span class="si">%i</span><span class="s2"> $\to$ </span><span class="si">%i</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">1</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">xlabel</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key_ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">)</span>
        <span class="n">fig_index</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fig</span>


<span class="k">def</span> <span class="nf">plot_dists_</span><span class="p">(</span><span class="n">val_dict</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;count&quot;</span><span class="p">,</span> <span class="n">use_kde</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_dict</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">columns</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">columns</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>
    <span class="n">fig_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">val_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
        <span class="n">key_ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">fig_index</span><span class="o">%</span><span class="k">columns</span>]
        <span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">val_dict</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">key_ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="n">stat</span><span class="p">,</span>
                     <span class="n">kde</span><span class="o">=</span><span class="n">use_kde</span> <span class="ow">and</span> <span class="p">((</span><span class="n">val_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">-</span><span class="n">val_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">())</span><span class="o">&gt;</span><span class="mf">1e-8</span><span class="p">))</span> <span class="c1"># Only plot kde if there is variance</span>
        <span class="n">key_ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> &quot;</span> <span class="o">+</span> <span class="p">(</span><span class="sa">r</span><span class="s2">&quot;(</span><span class="si">%i</span><span class="s2"> $\to$ </span><span class="si">%i</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">val_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">val_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">1</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">xlabel</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key_ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">)</span>
        <span class="n">fig_index</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fig</span>
 
<span class="c1">##############################################################</span>

<span class="k">def</span> <span class="nf">visualize_weight_distribution</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">):</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="c1"># if name.endswith(&quot;.bias&quot;):</span>
        <span class="c1">#     continue</span>
        <span class="k">if</span> <span class="s2">&quot;weight&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="c1"># key_name = f&quot;Layer {name.split(&#39;.&#39;)[1]}&quot;</span>
            <span class="n">key_name</span> <span class="o">=</span> <span class="n">name</span>
            <span class="c1"># weights[key_name] = param.detach().view(-1).cpu().numpy()</span>
            <span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">key_name</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
    
    <span class="c1">## Plotting</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plot_dists</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Weight vals&quot;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Weight distribution&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span> 
    
<span class="c1">##############################################################</span>
    
<span class="k">def</span> <span class="nf">visualize_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Inputs:</span>
<span class="sd">        net - Object of class BaseNetwork</span>
<span class="sd">        color - Color in which we want to visualize the histogram (for easier separation of activation functions)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">small_loader</span> <span class="o">=</span> <span class="n">dataloader</span>
    <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">small_loader</span><span class="p">))</span>
    <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="c1"># Pass one batch through the network, and calculate the gradients for the weights</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="c1"># Same as nn.CrossEntropyLoss, but as a function instead of module</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots</span>
    <span class="c1"># grads = {name: params.grad.view(-1).cpu().clone().numpy() for name, params in model.named_parameters() if &quot;weight&quot; in name}</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="p">[(</span><span class="n">name</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;weight&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">]</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    
    <span class="c1">## Plotting</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plot_dists</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Grad&quot;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Gradient distribution&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span> 
    
    <span class="k">if</span> <span class="n">print_variance</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">grads</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> - Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1">##############################################################</span>

<span class="k">def</span> <span class="nf">visualize_activations</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">small_loader</span> <span class="o">=</span> <span class="n">dataloader</span>
    <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">small_loader</span><span class="p">))</span>
    <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="c1"># Pass one batch through the network, and calculate the gradients for the weights</span>
    <span class="c1"># feats = imgs.view(imgs.shape[0], -1)</span>
    <span class="n">feats</span> <span class="o">=</span> <span class="n">imgs</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">layer_index</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">)):</span>
            <span class="n">feats</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">feats</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
                <span class="c1"># activations[f&quot;Layer {layer_index} ({type(layer).__name__})&quot;] = feats.view(-1).detach().cpu().numpy()</span>
                <span class="n">activations</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="sa">f</span><span class="s2">&quot;Layer </span><span class="si">{</span><span class="n">layer_index</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">,</span> <span class="n">feats</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
    
    <span class="c1">## Plotting</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plot_dists</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Activation vals&quot;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Activation distribution&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span> 
    
    <span class="k">if</span> <span class="n">print_variance</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">activations</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> - Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            
<span class="c1">##############################################################</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="initialization-functions">
<h4><span class="section-number">4.1.1.1.2. </span>Initialization Functions<a class="headerlink" href="#initialization-functions" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">const_init</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">var_init</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">xavier_uniform_init</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">init_layer</span><span class="p">(</span><span class="n">layer</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_layer</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">kaiming_uniform_init</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">init_layer</span><span class="p">(</span><span class="n">layer</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_layer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="model-and-dataset">
<h3><span class="section-number">4.1.1.2. </span>Model and Dataset<a class="headerlink" href="#model-and-dataset" title="Permalink to this headline">#</a></h3>
<p>For the purpose of making it run faster, we use a simple convnet rather than ResNet. And CIFAR10 is used as the dataset to train the model on.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc</span>
        <span class="p">])</span>
        
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">ToTensor</span>

<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">()),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">val_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">()),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Files already downloaded and verified
Files already downloaded and verified
</pre></div>
</div>
</div>
</div>
</section>
<section id="see-the-effect-of-different-intialization-methods">
<h3><span class="section-number">4.1.1.3. </span>See the Effect of Different Intialization Methods<a class="headerlink" href="#see-the-effect-of-different-intialization-methods" title="Permalink to this headline">#</a></h3>
<section id="constant-initialization">
<h4><span class="section-number">4.1.1.3.1. </span>Constant Initialization<a class="headerlink" href="#constant-initialization" title="Permalink to this headline">#</a></h4>
<p>Initializing all weights to a small number, the gradients are sparse and almost zero. Increasing the constant has little help, and the activation becomes too high. The model is not able to learn anything in this mode, until the weights get far away from the initialized values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">visualize_weights_gradients_activations</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">visualize_weight_distribution</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">visualize_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Now visualizing activations. It can take a bit...&quot;</span><span class="p">)</span>
    <span class="n">visualize_activations</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">print_variance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="n">const_init</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mf">0.005</span><span class="p">)</span>
<span class="n">visualize_weights_gradients_activations</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/initialization_9_0.png" src="_images/initialization_9_0.png" />
<img alt="_images/initialization_9_1.png" src="_images/initialization_9_1.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Now visualizing activations. It can take a bit...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "a164f968143643b5b9bbe1ed6b6b2d1c", "version_major": 2, "version_minor": 0}
</script><img alt="_images/initialization_9_4.png" src="_images/initialization_9_4.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Layer 0 (Conv2d) - Variance: 0.000905229477211833
Layer 2 (Conv2d) - Variance: 0.0004272360529284924
Layer 4 (Conv2d) - Variance: 0.00020684821356553584
Layer 7 (Linear) - Variance: 1.7030531580530806e-06
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="n">const_init</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">visualize_weights_gradients_activations</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/initialization_10_0.png" src="_images/initialization_10_0.png" />
<img alt="_images/initialization_10_1.png" src="_images/initialization_10_1.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Now visualizing activations. It can take a bit...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "77e98a5fd80a48d38ab16f7b87ccd30c", "version_major": 2, "version_minor": 0}
</script><img alt="_images/initialization_10_4.png" src="_images/initialization_10_4.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Layer 0 (Conv2d) - Variance: 36.20917892456055
Layer 2 (Conv2d) - Variance: 683578.125
Layer 4 (Conv2d) - Variance: 13238294528.0
Layer 7 (Linear) - Variance: 4359820148736.0
</pre></div>
</div>
</div>
</div>
<p>What if we draw random samples from a normal distribution? How much variance should the random initialization have?</p>
</section>
<section id="vanishing-gradients">
<h4><span class="section-number">4.1.1.3.2. </span>Vanishing Gradients<a class="headerlink" href="#vanishing-gradients" title="Permalink to this headline">#</a></h4>
<p>When the variance of initialized values is small, the gradients at  the earlier layers (0 and 2) are almost zero (1e-5). This can cause the model to learn very slowly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="n">var_init</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">visualize_weights_gradients_activations</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/initialization_12_0.png" src="_images/initialization_12_0.png" />
<img alt="_images/initialization_12_1.png" src="_images/initialization_12_1.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Now visualizing activations. It can take a bit...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "f984045a71fd471692d8873fd4160c94", "version_major": 2, "version_minor": 0}
</script><img alt="_images/initialization_12_4.png" src="_images/initialization_12_4.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Layer 0 (Conv2d) - Variance: 0.0007303644670173526
Layer 2 (Conv2d) - Variance: 7.553636532975361e-05
Layer 4 (Conv2d) - Variance: 9.660515206633136e-05
Layer 7 (Linear) - Variance: 0.00012513234105426818
</pre></div>
</div>
</div>
</div>
</section>
<section id="exploding-gradients-and-activation">
<h4><span class="section-number">4.1.1.3.3. </span>Exploding Gradients and Activation<a class="headerlink" href="#exploding-gradients-and-activation" title="Permalink to this headline">#</a></h4>
<p>On the other hand, if the variance of initialized values is large, the gradients also tend to have a large magnitude. The activation at the end of the network (e.g. layer 7) is very high. This problem is more prominent when we use deeper networks. This can cause the model to be high unstable and crashes due to NaN values.</p>
<p>It becomes clear we need to strike a balance, having a variance not too high and not too low. But how to find that balance? It will be hard to find one that gives us a good activation and gradient distribution across layers. And when the number of layers changes, you may need to tune the variance again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="n">var_init</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">visualize_weights_gradients_activations</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/initialization_14_0.png" src="_images/initialization_14_0.png" />
<img alt="_images/initialization_14_1.png" src="_images/initialization_14_1.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Now visualizing activations. It can take a bit...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "0454ba4ba4bf46c79c004a389175c941", "version_major": 2, "version_minor": 0}
</script><img alt="_images/initialization_14_4.png" src="_images/initialization_14_4.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Layer 0 (Conv2d) - Variance: 6.471884250640869
Layer 2 (Conv2d) - Variance: 446.0733642578125
Layer 4 (Conv2d) - Variance: 41183.9375
Layer 7 (Linear) - Variance: 692235.1875
</pre></div>
</div>
</div>
</div>
</section>
<section id="xavier-initialization">
<h4><span class="section-number">4.1.1.3.4. </span>Xavier Initialization<a class="headerlink" href="#xavier-initialization" title="Permalink to this headline">#</a></h4>
<p>Now let’s use Xavier initialization to handle the variance of weights automatically. As we can see, the gradient and activation distributions are much more reasonable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="n">xavier_uniform_init</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">visualize_weights_gradients_activations</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/initialization_16_0.png" src="_images/initialization_16_0.png" />
<img alt="_images/initialization_16_1.png" src="_images/initialization_16_1.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Now visualizing activations. It can take a bit...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "909fd6412ee944e8ab28b19990a48f4c", "version_major": 2, "version_minor": 0}
</script><img alt="_images/initialization_16_4.png" src="_images/initialization_16_4.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Layer 0 (Conv2d) - Variance: 0.10684434324502945
Layer 2 (Conv2d) - Variance: 0.022641852498054504
Layer 4 (Conv2d) - Variance: 0.0009143133647739887
Layer 7 (Linear) - Variance: 0.0007290925132110715
</pre></div>
</div>
</div>
</div>
</section>
<section id="kaiming-initialization">
<h4><span class="section-number">4.1.1.3.5. </span>Kaiming Initialization<a class="headerlink" href="#kaiming-initialization" title="Permalink to this headline">#</a></h4>
<p>Kaiming initialization is also a great choice:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="n">kaiming_uniform_init</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">visualize_weights_gradients_activations</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/initialization_18_0.png" src="_images/initialization_18_0.png" />
<img alt="_images/initialization_18_1.png" src="_images/initialization_18_1.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Now visualizing activations. It can take a bit...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "ffe74babdd3a416687ab4dea926f5d98", "version_major": 2, "version_minor": 0}
</script><img alt="_images/initialization_18_4.png" src="_images/initialization_18_4.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Layer 0 (Conv2d) - Variance: 0.5570183992385864
Layer 2 (Conv2d) - Variance: 0.16034908592700958
Layer 4 (Conv2d) - Variance: 0.1166415885090828
Layer 7 (Linear) - Variance: 0.24810270965099335
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="initialization-from-a-pre-trained-model-transfer-learning">
<h2><span class="section-number">4.1.2. </span>Initialization from a Pre-trained Model: Transfer Learning<a class="headerlink" href="#initialization-from-a-pre-trained-model-transfer-learning" title="Permalink to this headline">#</a></h2>
<p>Besides random initialization, there is another choice that is actually more commonly used: initialization from a pre-trained model. And it is better known name: “Transfer Learning”.</p>
<p>When using transfer learning, random initialization may still be used. As often times, the pre-trained model serves as the backbone of a new model you want to fine tune. The new model includes additional layers which still need to be intialized with reasonable random values.</p>
<p>With libraries like <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> and <code class="docutils literal notranslate"><span class="pre">timm</span></code>, it is easy to initialize a pre-trained model:</p>
<div class="docutils container" id="id2">
<dl class="footnote brackets">
<dt class="label" id="footcite-xavier"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Yee Whye Teh and D. Mike Titterington, editors, <em>AISTATS</em>, volume 9 of JMLR Proceedings, 249–256. JMLR.org, 2010. URL: <a class="reference external" href="http://dblp.uni-trier.de/db/journals/jmlr/jmlrp9.html#GlorotB10">http://dblp.uni-trier.de/db/journals/jmlr/jmlrp9.html#GlorotB10</a>.</p>
</dd>
</dl>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.models.resnet</span> <span class="kn">import</span> <span class="n">resnet18</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># This loads ImageNet weights.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter &#39;pretrained&#39; is deprecated since 0.13 and will be removed in 0.15, please use &#39;weights&#39; instead.
  warnings.warn(
/home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#39;weights&#39; are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
</pre></div>
</div>
</div>
</div>
</section>
<section id="showdown">
<h2><span class="section-number">4.1.3. </span>Showdown<a class="headerlink" href="#showdown" title="Permalink to this headline">#</a></h2>
<p>Now is our favorate part: the showdown of different initialization recipes. To save time, only a subset of training and validation data is used: controlled by <code class="docutils literal notranslate"><span class="pre">limit_train_batches</span></code> and <code class="docutils literal notranslate"><span class="pre">limit_val_batches</span></code> of the Trainer class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>pytorch-lightning<span class="o">==</span><span class="m">1</span>.6.4<span class="w"> </span>torchmetrics
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com
Requirement already satisfied: pytorch-lightning==1.6.4 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (1.6.4)
Requirement already satisfied: torchmetrics in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (0.9.3)
Requirement already satisfied: fsspec[http]!=2021.06.0,&gt;=2021.05.0 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from pytorch-lightning==1.6.4) (2022.7.1)
Requirement already satisfied: pyDeprecate&gt;=0.3.1 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from pytorch-lightning==1.6.4) (0.3.2)
Requirement already satisfied: torch&gt;=1.8.* in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from pytorch-lightning==1.6.4) (1.12.1)
Requirement already satisfied: numpy&gt;=1.17.2 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from pytorch-lightning==1.6.4) (1.23.2)
Requirement already satisfied: packaging&gt;=17.0 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from pytorch-lightning==1.6.4) (21.3)
Requirement already satisfied: protobuf&lt;=3.20.1 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from pytorch-lightning==1.6.4) (3.19.4)
Requirement already satisfied: tqdm&gt;=4.57.0 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from pytorch-lightning==1.6.4) (4.64.0)
Requirement already satisfied: typing-extensions&gt;=4.0.0 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from pytorch-lightning==1.6.4) (4.3.0)
Requirement already satisfied: PyYAML&gt;=5.4 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from pytorch-lightning==1.6.4) (6.0)
Requirement already satisfied: tensorboard&gt;=2.2.0 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from pytorch-lightning==1.6.4) (2.10.0)
Requirement already satisfied: aiohttp in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,&gt;=2021.05.0-&gt;pytorch-lightning==1.6.4) (3.8.1)
Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,&gt;=2021.05.0-&gt;pytorch-lightning==1.6.4) (2.28.1)
Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from packaging&gt;=17.0-&gt;pytorch-lightning==1.6.4) (3.0.9)
Requirement already satisfied: werkzeug&gt;=1.0.1 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (2.2.2)
Requirement already satisfied: absl-py&gt;=0.4 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (1.2.0)
Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (0.6.1)
Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (1.8.1)
Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (0.4.6)
Requirement already satisfied: grpcio&gt;=1.24.3 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (1.47.0)
Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (2.11.0)
Requirement already satisfied: wheel&gt;=0.26 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (0.37.1)
Requirement already satisfied: markdown&gt;=2.6.8 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (3.4.1)
Requirement already satisfied: setuptools&gt;=41.0.0 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (65.0.2)
Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (4.9)
Requirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (5.2.0)
Requirement already satisfied: six&gt;=1.9.0 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (1.16.0)
Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (0.2.8)
Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (1.3.1)
Requirement already satisfied: importlib-metadata&gt;=4.4 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from markdown&gt;=2.6.8-&gt;tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (4.12.0)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from requests-&gt;fsspec[http]!=2021.06.0,&gt;=2021.05.0-&gt;pytorch-lightning==1.6.4) (3.3)
Requirement already satisfied: certifi&gt;=2017.4.17 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from requests-&gt;fsspec[http]!=2021.06.0,&gt;=2021.05.0-&gt;pytorch-lightning==1.6.4) (2022.6.15)
Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from requests-&gt;fsspec[http]!=2021.06.0,&gt;=2021.05.0-&gt;pytorch-lightning==1.6.4) (2.1.0)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from requests-&gt;fsspec[http]!=2021.06.0,&gt;=2021.05.0-&gt;pytorch-lightning==1.6.4) (1.26.11)
Requirement already satisfied: MarkupSafe&gt;=2.1.1 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (2.1.1)
Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from aiohttp-&gt;fsspec[http]!=2021.06.0,&gt;=2021.05.0-&gt;pytorch-lightning==1.6.4) (1.8.1)
Requirement already satisfied: frozenlist&gt;=1.1.1 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from aiohttp-&gt;fsspec[http]!=2021.06.0,&gt;=2021.05.0-&gt;pytorch-lightning==1.6.4) (1.3.1)
Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from aiohttp-&gt;fsspec[http]!=2021.06.0,&gt;=2021.05.0-&gt;pytorch-lightning==1.6.4) (6.0.2)
Requirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from aiohttp-&gt;fsspec[http]!=2021.06.0,&gt;=2021.05.0-&gt;pytorch-lightning==1.6.4) (4.0.2)
Requirement already satisfied: aiosignal&gt;=1.1.2 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from aiohttp-&gt;fsspec[http]!=2021.06.0,&gt;=2021.05.0-&gt;pytorch-lightning==1.6.4) (1.2.0)
Requirement already satisfied: attrs&gt;=17.3.0 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from aiohttp-&gt;fsspec[http]!=2021.06.0,&gt;=2021.05.0-&gt;pytorch-lightning==1.6.4) (22.1.0)
Requirement already satisfied: zipp&gt;=0.5 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from importlib-metadata&gt;=4.4-&gt;markdown&gt;=2.6.8-&gt;tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (3.8.1)
Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (0.4.8)
Requirement already satisfied: oauthlib&gt;=3.0.0 in /home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&gt;=2.2.0-&gt;pytorch-lightning==1.6.4) (3.2.0)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>
<span class="kn">from</span> <span class="nn">torchmetrics</span> <span class="kn">import</span> <span class="n">Accuracy</span>


<span class="k">class</span> <span class="nc">ImageClassifier</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">Accuracy</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># return torch.optim.Adam(self.parameters(), lr=0.001)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c1"># Note: The return value can be None, a loss tensor, or a dictionary with a &quot;loss&quot; key.</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>
    
    <span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val_accuracy</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">preds</span><span class="o">=</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">val_loss</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning.loggers</span> <span class="kn">import</span> <span class="n">CSVLogger</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">seed_everything</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.callbacks</span> <span class="kn">import</span> <span class="n">Callback</span><span class="p">,</span> <span class="n">EarlyStopping</span>


<span class="k">class</span> <span class="nc">ImageClassifierCallback</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">def</span> <span class="nf">on_train_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_train_batch_end</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">on_validation_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_validation_batch_end</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;val_acc&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">pl_module</span><span class="o">.</span><span class="n">val_accuracy</span><span class="o">.</span><span class="n">compute</span><span class="p">(),</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>
    
    <span class="k">def</span> <span class="nf">on_validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_validation_epoch_end</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">avg_acc</span> <span class="o">=</span> <span class="n">pl_module</span><span class="o">.</span><span class="n">val_accuracy</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;val_acc&#39;</span><span class="p">,</span> <span class="n">avg_acc</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span><span class="p">,</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="s2">&quot;val_accuracy:&quot;</span><span class="p">,</span> <span class="n">avg_acc</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">+=</span> <span class="mi">1</span>


<span class="k">def</span> <span class="nf">create_trainer</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;image_classifier&quot;</span><span class="p">):</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
        <span class="n">limit_train_batches</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
        <span class="n">limit_val_batches</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">max_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">logger</span><span class="o">=</span><span class="n">CSVLogger</span><span class="p">(</span><span class="n">save_dir</span><span class="o">=</span><span class="s2">&quot;csv_logs&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">),</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">ImageClassifierCallback</span><span class="p">(),</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_acc&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">)],</span>
        <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">trainer</span>
</pre></div>
</div>
</div>
</div>
<section id="initialize-from-a-pre-trained-model">
<h3><span class="section-number">4.1.3.1. </span>Initialize from a Pre-trained Model<a class="headerlink" href="#initialize-from-a-pre-trained-model" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torchvision</span>

<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ImageClassifier</span><span class="p">(</span><span class="n">net</span><span class="o">=</span><span class="n">net</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">create_trainer</span><span class="p">(</span><span class="s2">&quot;pretrained&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter &#39;pretrained&#39; is deprecated since 0.13 and will be removed in 0.15, please use &#39;weights&#39; instead.
  warnings.warn(
/home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#39;weights&#39; are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: csv_logs/pretrained
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name         | Type     | Params
------------------------------------------
0 | net          | ResNet   | 11.2 M
1 | val_accuracy | Accuracy | 0     
------------------------------------------
11.2 M    Trainable params
0         Non-trainable params
11.2 M    Total params
44.727    Total estimated model params size (MB)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "44066250bb2e4333b651f7dc9cc2c01a", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 0 -- val_accuracy: 0.109375
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "c0b5b6d7c12e4ec1a6e960177f66dd18", "version_major": 2, "version_minor": 0}
</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages/pytorch_lightning/callbacks/progress/base.py:227: UserWarning: The progress bar already tracks a metric with the name(s) &#39;loss&#39; and `self.log(&#39;loss&#39;, ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.
  rank_zero_warn(
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "5fe3c8f047784d7e851b063eb235076b", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 1 -- val_accuracy: 0.6246936321258545
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "d28fcc2195ae4c43b05a1cb7ca05b94c", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 2 -- val_accuracy: 0.6432549357414246
</pre></div>
</div>
</div>
</div>
</section>
<section id="random-kaiming-initialization">
<h3><span class="section-number">4.1.3.2. </span>Random Kaiming Initialization<a class="headerlink" href="#random-kaiming-initialization" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ImageClassifier</span><span class="p">(</span><span class="n">net</span><span class="o">=</span><span class="n">net</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">create_trainer</span><span class="p">(</span><span class="s2">&quot;kaiming&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter &#39;pretrained&#39; is deprecated since 0.13 and will be removed in 0.15, please use &#39;weights&#39; instead.
  warnings.warn(
/home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#39;weights&#39; are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: csv_logs/kaiming
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name         | Type     | Params
------------------------------------------
0 | net          | ResNet   | 11.2 M
1 | val_accuracy | Accuracy | 0     
------------------------------------------
11.2 M    Trainable params
0         Non-trainable params
11.2 M    Total params
44.727    Total estimated model params size (MB)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "75ff55925f144e6693d6ec20b3e66c4a", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 0 -- val_accuracy: 0.15625
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "efa3a6bfd0c04a3db4b517fef79ec443", "version_major": 2, "version_minor": 0}
</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/ubuntu/anaconda3/envs/p38/lib/python3.8/site-packages/pytorch_lightning/callbacks/progress/base.py:227: UserWarning: The progress bar already tracks a metric with the name(s) &#39;loss&#39; and `self.log(&#39;loss&#39;, ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_metrics()` in the progress bar callback.
  rank_zero_warn(
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "d51c823555b34a3eb20b9bd5282601f6", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 1 -- val_accuracy: 0.3893994987010956
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "d0eb8f5083e44768a9dc7cbfe2679ab6", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 2 -- val_accuracy: 0.4125928282737732
</pre></div>
</div>
</div>
</div>
</section>
<section id="random-xavier-initialization">
<h3><span class="section-number">4.1.3.3. </span>Random Xavier Initialization<a class="headerlink" href="#random-xavier-initialization" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">xavier_uniform_init</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ImageClassifier</span><span class="p">(</span><span class="n">net</span><span class="o">=</span><span class="n">net</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">create_trainer</span><span class="p">(</span><span class="s2">&quot;xavier&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: csv_logs/xavier
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name         | Type     | Params
------------------------------------------
0 | net          | ResNet   | 11.2 M
1 | val_accuracy | Accuracy | 0     
------------------------------------------
11.2 M    Trainable params
0         Non-trainable params
11.2 M    Total params
44.727    Total estimated model params size (MB)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "e125e0798d98470c82795d3c138f548a", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 0 -- val_accuracy: 0.09375
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "26a23ffd9c464ff0aa21d01032fdbe7c", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "c5023bf6cb45477baba7b4fdf338dbc4", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 1 -- val_accuracy: 0.40410539507865906
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "e3a5713dd0fd4649bfa36dc73a111c93", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 2 -- val_accuracy: 0.41878095269203186
</pre></div>
</div>
</div>
</div>
</section>
<section id="id3">
<h3><span class="section-number">4.1.3.4. </span>Constant Initialization<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">const_init</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mf">0.005</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ImageClassifier</span><span class="p">(</span><span class="n">net</span><span class="o">=</span><span class="n">net</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">create_trainer</span><span class="p">(</span><span class="s2">&quot;const&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: csv_logs/const
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name         | Type     | Params
------------------------------------------
0 | net          | ResNet   | 11.2 M
1 | val_accuracy | Accuracy | 0     
------------------------------------------
11.2 M    Trainable params
0         Non-trainable params
11.2 M    Total params
44.727    Total estimated model params size (MB)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "a108c8cd43fa40728335223459f43361", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 0 -- val_accuracy: 0.09375
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "6a1d50ac0f6f40cc83b0af8d939e27f8", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "79b025140d8d493f9b58888455c72045", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 1 -- val_accuracy: 0.14981617033481598
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "fda5486e20834741b0520571cbe605e6", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 2 -- val_accuracy: 0.15083539485931396
</pre></div>
</div>
</div>
</div>
</section>
<section id="normal-distribution-initialization-small-variance">
<h3><span class="section-number">4.1.3.5. </span>Normal Distribution Initialization (Small Variance)<a class="headerlink" href="#normal-distribution-initialization-small-variance" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">var_init</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ImageClassifier</span><span class="p">(</span><span class="n">net</span><span class="o">=</span><span class="n">net</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">create_trainer</span><span class="p">(</span><span class="s2">&quot;normal_small&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name         | Type     | Params
------------------------------------------
0 | net          | ResNet   | 11.2 M
1 | val_accuracy | Accuracy | 0     
------------------------------------------
11.2 M    Trainable params
0         Non-trainable params
11.2 M    Total params
44.727    Total estimated model params size (MB)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "146f39800e334ce0979e3c2b0ebd58ac", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 0 -- val_accuracy: 0.140625
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "70f30ea3da004d6393484fbedb33c8c3", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "05618bf9eb4e46699a9b555cd20b43e9", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 1 -- val_accuracy: 0.10569852590560913
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "a1543aefb0a24f2db7f9e8c02a9bf7db", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 2 -- val_accuracy: 0.1053527221083641
</pre></div>
</div>
</div>
</div>
</section>
<section id="normal-distribution-initialization-large-variance">
<h3><span class="section-number">4.1.3.6. </span>Normal Distribution Initialization (Large Variance)<a class="headerlink" href="#normal-distribution-initialization-large-variance" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">var_init</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ImageClassifier</span><span class="p">(</span><span class="n">net</span><span class="o">=</span><span class="n">net</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">create_trainer</span><span class="p">(</span><span class="s2">&quot;normal_large&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: csv_logs/normal_large
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name         | Type     | Params
------------------------------------------
0 | net          | ResNet   | 11.2 M
1 | val_accuracy | Accuracy | 0     
------------------------------------------
11.2 M    Trainable params
0         Non-trainable params
11.2 M    Total params
44.727    Total estimated model params size (MB)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "0910f8a7d0f540d0aff4f89763d8afd3", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 0 -- val_accuracy: 0.140625
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "6e77eb8baeab4e129985e8a01327cd66", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "82efbe54aa694231a7865ed3a23b48df", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 1 -- val_accuracy: 0.25
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "70f3d51b0e9e489f82c3a7768a945aaf", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 2 -- val_accuracy: 0.2776918411254883
</pre></div>
</div>
</div>
</div>
<hr class="footnotes docutils" />
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="network_weights.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">4. </span>Knobs for Network Weights: Initialization and Optimizers</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="optimizer.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4.2. </span>Effect of Optimizers</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By KUNGFU.AI<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>